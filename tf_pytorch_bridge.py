# -*- coding: utf-8 -*-
"""TF_pyTorch_bridge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Al_fX-xJcZlu2pc7KUblF1GuUxODkfOU
"""

import tensorflow as tf

import torch as th

session = tf.InteractiveSession()

a = tf.placeholder(tf.float32, name='a')

b = tf.placeholder(tf.float32, name='b')

c = 3*a+4*b*b

session.run(c,{a: 1, b:3})

def tensorflow_function(_tf_session, _input_tensors, _output_tensor, dtype=tf.float32):  
  # create gradient placeholders\
  _output_tensors = [_output_tensor]
  _gradient_placeholders = [tf.placeholder(dtype=dtype,name=f'gradient{i}') for i, _ in enumerate(_output_tensors)]
  _gradients = tf.gradients(
        ys=_output_tensors, xs=_input_tensors, grad_ys=_gradient_placeholders, unconnected_gradients='zero'
    )
  
  class TensorFlowFunction(th.autograd.Function):
    @staticmethod
    def forward(ctx, *args):
      assert len(args) == len(_input_tensors)
            
      feed_dict = {input: value.detach().numpy() for input, value in zip(_input_tensors, args)}
      outputs = _tf_session.run(_output_tensors, feed_dict) 

      tensors = [th.as_tensor(output) for output in outputs]
      
      ctx.save_for_backward(*args, *tensors)
      
      return tensors[0]

    @staticmethod
    def backward(ctx, *grad_outputs):
      assert len(grad_outputs) == len(_gradient_placeholders)
      inputs = ctx.saved_tensors[:len(_input_tensors)]
      outputs = ctx.saved_tensors[-len(_output_tensors):]

      feed_dict = {}
      feed_dict.update({input: value.detach().numpy() for input, value in zip(_input_tensors, inputs)})
      feed_dict.update({gradient_output: value.detach().numpy() for gradient_output, value in zip(_gradient_placeholders, grad_outputs)})

      gradients = _tf_session.run(_gradients, feed_dict)      
      # Need to return a tuple or pytorch will misunderstand it and only see one tensor ("of tensors")
      tensors = tuple(th.as_tensor(gradient) for gradient in gradients)
      return tuple(tensors)
    
  return TensorFlowFunction()

f = tensorflow_function(session, [a,b], c)

a_ = th.tensor(1, dtype=th.float32, requires_grad=True)
b_ = th.tensor(3, dtype=th.float32, requires_grad=True)
x = f.apply(a_,b_)

x

x.backward()

a_.grad

b_.grad

# from https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342
import tensorflow as tf
from tensorflow.python.framework import ops
import numpy as np

# Define custom py_func which takes also a grad op as argument:
def py_func_w_grad(func, inp, Tout, name=None, grad=None):
    
    # Need to generate a unique name to avoid duplicates:
    id = str(np.random.randint(0, 1E+8))
    rnd_name = 'PyFunc' + id
    
    func_node = tf.numpy_function(func, inp, Tout, name=rnd_name)
    print(rnd_name)
    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example
    return func_node

def pytorch_function(func, inp, Tout, name=None):
  def compute(*inputs):
    #input_tensors = [th.tensor(input.numpy(), requires_grad=True) for input in inputs]
    #output = func(*inputs)
    
    print('hello')
    
    def compute_grad(d_output):
      print('world')
      #d_output_tensor = th.tensor(d_output.numpy(), requires_grad=False)
      #grad_tensors = th.autograd.grad([output], inputs, grad_outputs=[d_output_tensor], allow_unused=True)  
      #grads = [tf.convert_to_tensor(tensor) for tensor in grad_tensors]
      #return grads
      return inputs
    
    #return tf.convert_to_tensor(output, preferred_dtype=Tout), grad
    return inputs[1]*4, compute_grad
  
  return tf.py_function(tf.custom_gradient(compute), inp, Tout, name=name)

@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.log(1 + e), grad

grad_log1pexp = tfe.gradients_function(log1pexp)

# As before, the gradient computation works fine at x = 0.
grad_log1pexp(0.)[0].eval()

def pytorch_expr(a,b):
  return 3*a+4*b*b

c_tf = pytorch_function(pytorch_expr, [a, b], tf.float32)

c_tf

session.run(c_tf,{a: 1, b:3})

c_tf_grad = tf.gradients([a,b], [c_tf],  unconnected_gradients='zero')

session.run([c_tf, c_tf_grad[0]], {a: 1, b:3})

session.graph

tfe = tf.contrib.eager

tfe.gradients_function(c_tf)(1, 4)

